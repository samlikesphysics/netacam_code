{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpy36conda3f57a39a474d40cab2a06366e779d487",
   "display_name": "Python 3.6.10 64-bit ('py36': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scripts\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import stoclust as sc\n",
    "import matplotlib.lines as ln\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import scipy.linalg as la\n",
    "import os"
   ]
  },
  {
   "source": [
    "# Overview"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The technical coefficients $C_{ri,sj}$, whether they describe the GTAP 8 dataset or the null model, do not *completely* describe the model. Rather, they only describe the likelihood of sector $s,j$ spending a given dollar on sector $r,i$. To complete the model we need a monetary distribution. It is sufficient to specify the spending distribution across final demand sectors: $x_{ra}$. This may be retrieve from the GTAP 8 dataset for the empirical case; for the null model, it must be generated, and in a way such that the regional deficit is controlled. How this is done precisely is described in the supplementary material,but it depends upon the technical coefficients $C_{ri,sj}$ and their Leontief inverse $\\mathbf{L} = (\\mathbf{I} - \\mathbf{C})^{-1}$, which have already been calculated (see `inverse.ipynb` in this repo).\n",
    "The regional spending distribution $\\hat{x}_r$ is determined as a Dirichlet centered on the eigenvector $\\pi_{r}$ of the matrix\n",
    "$$\n",
    "K_{rs} = \\sum_{i,j,a} U_{ri}(\\mathbf{I}-\\mathbf{C})^{-1}_{ri,sj}c_{j,sa}s_{a,s}\n",
    "$$\n",
    "where $U_{ri}$ corresponds to the `FD_L` block of the $\\mathbf{L}$ matrix, $c_{j,sa}$ to the `DC_L` block, and $s_{a,s}$ is itself randomly generated from a uniform Dirichlet (this quantity describes the distribution of spending among a given region's final demand sectors). With $\\hat{x}_r$ and $s_{a,r}$ in hand, the demand vector $d_{ri}$ can be described as $d_{ri} = \\sum_a c_{i,ra}s_{a,r}\\hat{x}_r$. With $\\mathbf{d}$ and $\\mathbf{C}$, the attribution matrix can be readily calculated by\n",
    "$$\n",
    "\\hat{A}_{ri,s} = \\frac{\\sum_j(\\mathbf{I}-\\mathbf{C})^{-1}_{ri,sj}d_{sj}}{\\sum_{s,j}(\\mathbf{I}-\\mathbf{C})^{-1}_{ri,sj}d_{sj}}\n",
    "$$\n",
    "From the attribution matrix and an impact distribution $\\hat{e}_{ri}$ we can calculate the attributed distribution $\\hat{a}_s$.\n",
    "\n",
    "To get the impact distributions for $\\mathrm{CO}_2$ and labor, we draw estimated carbon production and labor-time from the GTAP 8 and GMig2 datasets. The impact distribution for the null model, termed \"unobtainium,\" is generated according to a Dirichlet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First we load in the GTAP data, the generated blocks (null and GTAP 8), and their Leontief inverses."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtap = scripts.GTAP()\n",
    "num_reg = gtap.regions.items.size\n",
    "num_commod = gtap.commodities.items.size\n",
    "num_fac = gtap.factors.items.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reg = gtap.megaregions.at_scale(1)\n",
    "new_commod = gtap.commodities.at_scale(2)\n",
    "new_fac = gtap.factors.at_scale(2)\n",
    "\n",
    "num_new_reg = new_reg.clusters.size\n",
    "num_new_commod = new_commod.clusters.size\n",
    "num_new_fac = new_fac.clusters.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = sc.Group(['D','M','F','C'])\n",
    "\n",
    "T_null = {\n",
    "    names[i]+names[j]: np.load('scripts/data/computed/blocks/null/'+names[i]+names[j]+'_T.npy')\n",
    "    for i in range(len(names))\n",
    "    for j in range(len(names))\n",
    "}\n",
    "\n",
    "T_ind = {\n",
    "    names[i]+names[j]: np.load('scripts/data/computed/blocks/industry/'+names[i]+names[j]+'_T.npy')\n",
    "    for i in range(len(names))\n",
    "    for j in range(len(names))\n",
    "}\n",
    "\n",
    "L_null = {\n",
    "    names[i]+names[j]: np.load('scripts/data/computed/leontief/null/'+names[i]+names[j]+'_L.npy')\n",
    "    for i in range(len(names))\n",
    "    for j in range(len(names))\n",
    "}\n",
    "\n",
    "L_ind = {\n",
    "    names[i]+names[j]: np.load('scripts/data/computed/leontief/industry/'+names[i]+names[j]+'_L.npy')\n",
    "    for i in range(len(names))\n",
    "    for j in range(len(names))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = T_null['CC'].shape[0]\n",
    "\n",
    "Flow_nulls = [\n",
    "    [],[],[],[]\n",
    "]\n",
    "\n",
    "for k in tqdm(range(num_samples)):\n",
    "    for i in range(4):\n",
    "        Flow_nulls[i].append(\n",
    "            sum([L_null[names[i]+n][k]@T_null[n+'C'][k] for n in names])\n",
    "        )\n",
    "Flow_null = {\n",
    "    names[i]: np.array(Flow_nulls[i])\n",
    "    for i in range(len(names))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flow_ind = {\n",
    "    names[i]: \n",
    "    sum([L_ind[names[i]+n]@T_ind[n+'C'] for n in names])\n",
    "    for i in tqdm(range(len(names)))\n",
    "}"
   ]
  },
  {
   "source": [
    "# Setting incomes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here we generate the regional income distributions $\\hat{x}_r$ for the empirical and null models, which will be necessary for calculating the attribution matrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_spending = sc.utils.stoch(gtap.fetch_sam(demand=gtap.sectors['final_demand']).sum(axis=1).reshape([num_reg*3]))\n",
    "\n",
    "ind_income = Flow_ind['F']@ind_spending\n",
    "\n",
    "ind_activity = Flow_ind['D']@ind_spending\n",
    "\n",
    "ind_value_added = T_ind['FD'].sum(axis=0)*ind_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_nat_spending = sc.utils.stoch(gtap.fetch_sam(demand=gtap.sectors['final_demand']).sum(axis=1),axis=1).reshape([num_reg*3])\n",
    "ind_flow_T = (Flow_ind['F']*ind_nat_spending[None,:]).reshape([num_reg,num_fac,num_reg,3]).sum(axis=(1,3))\n",
    "eig,vec = la.eig(ind_flow_T)\n",
    "ind_Y0 = np.real(sc.utils.stoch(vec[:,np.argmax(np.abs(eig))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_spending_reg = ind_spending.reshape([num_reg,3]).sum(axis=1)"
   ]
  },
  {
   "source": [
    "The Dirichlet used to calculate the null model income distribution needs a scale parameter, which we get from the corresponding Kullback-Liebler divergence in the empirical model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import kl_div\n",
    "alpha_0 = num_reg*(1/kl_div(ind_spending_reg,ind_Y0).sum())/num_new_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([alpha_0]*4)\n",
    "#alpha = alpha/np.array([1,2,4,8])\n",
    "null_spending = []\n",
    "for k in tqdm(range(num_samples)):\n",
    "    nat_spending = np.random.dirichlet((1,)*3,size=num_new_reg).reshape([num_new_reg,3])\n",
    "    nat_flow = (Flow_null['F'][k].reshape([num_new_reg,num_new_fac,num_new_reg,3]).sum(axis=1)*nat_spending[None,:,:]).sum(axis=2)\n",
    "    eig,vec = la.eig(nat_flow)\n",
    "    Y0 = np.real(sc.utils.stoch(vec[:,np.argmax(np.abs(eig))]))\n",
    "    X = np.random.dirichlet(alpha[int(k/250)]*Y0)\n",
    "    null_spending.append((X[:,None]*nat_spending).reshape(num_new_reg*3))\n",
    "null_spending = np.array(null_spending)\n",
    "null_activity = (null_spending[:,None,:]*Flow_null['D']).sum(axis=2)\n",
    "null_income = (null_spending[:,None,:]*Flow_null['F']).sum(axis=2)\n",
    "null_value_added = T_null['FD'].sum(axis=1)*null_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = null_spending.reshape([1000,num_new_reg,3]).sum(axis=2)\n",
    "Y = null_income.reshape([1000,num_new_reg,num_new_fac]).sum(axis=2)\n",
    "from scipy.special import kl_div\n",
    "j = 0\n",
    "kl_div(X,Y)[j*200:(j+1)*200].sum()/200"
   ]
  },
  {
   "source": [
    "With the acquired information we can calculate the attribution matrices for the null and empirical models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attr_D_null = (Flow_null['D']*null_spending[:,None,:]/(null_activity+(null_activity==0))[:,:,None])\n",
    "Attr_D_ind = Flow_ind['D']*ind_spending[None,:]/(ind_activity+(ind_activity==0))[:,None]"
   ]
  },
  {
   "source": [
    "# Distributing Intensities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We draw up the GTAP data on labor and carbon dioxide impact distributions. These are used to derive the intensities within each region; for the null models, these intensities are held fixed to their empirical value, while the impact distributions are adjusted to match the economic activity levels generated by the null model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = gtap.fetch_labor().sum(axis=0)\n",
    "\n",
    "FD_T_big = T_ind['FD'].reshape([num_reg,num_fac,num_reg*num_commod])[:,gtap.factors['Labor'].in_superset,:]\n",
    "ind_income_big = ind_income.reshape([num_reg,num_fac])[:,gtap.factors['Labor'].in_superset]\n",
    "lab_ind = (FD_T_big*lab[:,:,None]*ind_activity[None,None,:]/ind_income_big[:,:,None]).sum(axis=(0,1))\n",
    "lab_f_ind = (lab_ind+(ind_value_added==0))/(ind_value_added+(ind_value_added==0))\n",
    "\n",
    "lab_agg = (gtap.commodities.measure(\n",
    "    gtap.megaregions.measure(\n",
    "        (lab_ind).reshape([num_reg,num_commod]),\n",
    "        axis=0\n",
    "    )[new_reg.clusters.in_superset],\n",
    "    axis=1\n",
    ")[:,new_commod.clusters.in_superset]).reshape(num_new_reg*num_new_commod)\n",
    "\n",
    "val_agg = (gtap.commodities.measure(\n",
    "    gtap.megaregions.measure(\n",
    "        (ind_value_added).reshape([num_reg,num_commod]),\n",
    "        axis=0\n",
    "    )[new_reg.clusters.in_superset],\n",
    "    axis=1\n",
    ")[:,new_commod.clusters.in_superset]).reshape(num_new_reg*num_new_commod)\n",
    "\n",
    "lab_f_null = (lab_agg+(val_agg==0))/(val_agg+(val_agg==0))\n",
    "lab_null = lab_f_null[None,:]*null_value_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_ind = gtap.fetch_carbon(demand=gtap.sectors['activities']).sum(axis=1).reshape([num_reg*num_commod])\n",
    "co2_f_ind = (co2_ind+(ind_value_added==0))/(ind_value_added+(ind_value_added==0))\n",
    "\n",
    "co2_agg = (gtap.commodities.measure(\n",
    "    gtap.megaregions.measure(\n",
    "        (co2_ind).reshape([num_reg,num_commod]),\n",
    "        axis=0\n",
    "    )[new_reg.clusters.in_superset],\n",
    "    axis=1\n",
    ")[:,new_commod.clusters.in_superset]).reshape(num_new_reg*num_new_commod)\n",
    "\n",
    "co2_f_null = (co2_agg+(val_agg==0))/(val_agg+(val_agg==0))\n",
    "co2_null = co2_f_null[None,:]*null_value_added"
   ]
  },
  {
   "source": [
    "Here, we generate the null model for the distribution of \"unobtainium,\" a hypothetical resource whose extraction has a measurable ecological impact. \n",
    "\n",
    "NOTE: Run the next cell to generate for a fixed heterogeneity. To vary the heterogeneity, run the one after it. Once either cell is run, run the third cell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "unb_f_ind = np.random.dirichlet((alpha,)*num_reg*num_commod,size=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = np.array([1,2,4,8])\n",
    "unb_f_inds = []\n",
    "for j in tqdm(range(num_samples)):\n",
    "    unb_f_inds.append(np.random.dirichlet((param[int(j/250)]*alpha,)*num_reg*num_commod,))\n",
    "unb_f_ind = np.array([unb_f_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unb_ind = ind_value_added[None,:]*unb_f_ind\n",
    "\n",
    "unb_agg = (gtap.commodities.measure(\n",
    "    gtap.megaregions.measure(\n",
    "        (unb_ind).reshape([num_samples,num_reg,num_commod]),\n",
    "        axis=1\n",
    "    )[:,new_reg.clusters.in_superset],\n",
    "    axis=2\n",
    ")[:,:,new_commod.clusters.in_superset]).reshape(num_samples,num_new_reg*num_new_commod)\n",
    "\n",
    "unb_f_null = (unb_agg+(val_agg==0)[None,:])/(val_agg[None,:]+(val_agg[None,:]==0))\n",
    "unb_null = unb_f_null[:,None,:]*null_value_added[None,:,:]"
   ]
  },
  {
   "source": [
    "With our impact distributions set, we pass each through the attribution matrix to get the attributed impact distribution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_flows_ind = lab_ind[:,None]*Attr_D_ind\n",
    "co2_flows_ind = co2_ind[:,None]*Attr_D_ind\n",
    "val_flows_ind = ind_value_added[:,None]*Attr_D_ind\n",
    "\n",
    "lab_flows_null = lab_null[:,:,None]*Attr_D_null[:,:,:]\n",
    "co2_flows_null = co2_null[:,:,None]*Attr_D_null[:,:,:]\n",
    "val_flows_null = null_value_added[:,:,None]*Attr_D_null[:,:,:]\n",
    "\n",
    "unb_attr_null = np.zeros([num_samples,num_samples,3*num_new_reg])\n",
    "for k in tqdm(range(num_samples)):\n",
    "    unb_attr_null[k,:,:] += (unb_null[k,:,:,None]*Attr_D_null).sum(axis=1)"
   ]
  },
  {
   "source": [
    "We save the data. Use the name `null_unobtainium` for the varying heterogeneity ensemble, `null_deficit` for the varying deficit ensemble, `null` for fixed parameters and `industry` for GTAP 8 data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_type = 'null_unobtainium'\n",
    "\n",
    "if not os.path.isdir(os.path.abspath('scripts/data/computed/flows')):\n",
    "    os.mkdir(os.path.abspath('scripts/data/computed/flows'))\n",
    "if not os.path.isdir(os.path.abspath('scripts/data/computed/flows/'+null_type)):\n",
    "    os.mkdir(os.path.abspath('scripts/data/computed/flows/'+null_type))\n",
    "if not os.path.isdir(os.path.abspath('scripts/data/computed/flows/industry')):\n",
    "    os.mkdir(os.path.abspath('scripts/data/computed/flows/industry'))\n",
    "\n",
    "np.save('scripts/data/computed/flows/'+null_type+'/labor.npy',lab_flows_null)\n",
    "np.save('scripts/data/computed/flows/'+null_type+'/carbon.npy',co2_flows_null)\n",
    "np.save('scripts/data/computed/flows/'+null_type+'/values.npy',val_flows_null)\n",
    "np.save('scripts/data/computed/flows/'+null_type+'/unobtainium_in.npy',unb_null)\n",
    "np.save('scripts/data/computed/flows/'+null_type+'/unobtainium_out.npy',unb_attr_null)"
   ]
  }
 ]
}