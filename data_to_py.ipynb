{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from stoclust import Group, Hierarchy\n",
    "from functools import reduce\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.abspath('scripts/data')):\n",
    "    os.mkdir(os.path.abspath('scripts/data'))\n",
    "\n",
    "if not os.path.isdir(os.path.abspath('scripts/data/ID')):\n",
    "    os.mkdir(os.path.abspath('scripts/data/ID'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "source": [
    "First we load the data from the CSV files we stored it in. To start with, we load the names of the variables (sectors, regions, commodities, factors, etc.)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commodities = Group(np.loadtxt(open(\"data/H2.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1])\n",
    "regions = Group(np.loadtxt(open(\"data/H1.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1])\n",
    "sectors = Group(np.loadtxt(open(\"data/SSET.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1])\n",
    "np.save('scripts/data/ID/commodities.npy',commodities.elements)\n",
    "np.save('scripts/data/ID/regions.npy',regions.elements)\n",
    "\n",
    "\n",
    "factors = Group(np.loadtxt(open(\"data/H6.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1])\n",
    "land_factors = Group(np.loadtxt(open(\"data/AEZS.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1],superset=factors)\n",
    "lab_factors = Group(['UnSkLab','SkLab'],superset=factors)\n",
    "factors_hier = Hierarchy(\n",
    "    factors,\n",
    "    Group(list(factors)+['Land']+['Labor']),\n",
    "    {\n",
    "        factors.size + 0 : (1,land_factors.in_superset),\n",
    "        factors.size + 1 : (2,lab_factors.in_superset)\n",
    "    }\n",
    ")\n",
    "orig_factors = factors_hier.at_scale(1)\n",
    "orig_factors.clusters.set_super(sectors)\n",
    "np.save('scripts/data/ID/factors.npy',factors.elements)\n",
    "np.save('scripts/data/ID/factors_land.npy',land_factors.elements)\n",
    "np.save('scripts/data/ID/factors_lab.npy',lab_factors.elements)\n",
    "\n",
    "ergs = Group(np.loadtxt(open(\"data/EC.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1],superset=commodities)\n",
    "fuels = Group(np.loadtxt(open(\"data/FC.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1],superset=ergs)\n",
    "ag = Group(np.loadtxt(open(\"data/MLND.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1],superset=commodities)\n",
    "commodities_hier = Hierarchy(\n",
    "    commodities,\n",
    "    Group(list(commodities)+['Fuels']+['Agriculture']+['Energy']),\n",
    "    {\n",
    "        commodities.size + 0 : (1,ergs.in_superset[fuels.in_superset]),\n",
    "        commodities.size + 1 : (1,ag.in_superset),\n",
    "        commodities.size + 2 : (2,np.array([commodities.size + 0,ergs.in_superset[ergs.ind['ely']]]))\n",
    "    }\n",
    ")\n",
    "np.save('scripts/data/ID/commodities_energy.npy',ergs.elements)\n",
    "np.save('scripts/data/ID/commodities_fuel.npy',fuels.elements)\n",
    "np.save('scripts/data/ID/commodities_ag.npy',ag.elements)\n",
    "\n",
    "covers = Group(np.loadtxt(open(\"data/COVS.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:,1])\n",
    "np.save('scripts/data/ID/covers.npy',covers.elements)\n",
    "\n",
    "imports = Group(['m_'+c for c in commodities], superset=sectors)\n",
    "activities = Group(['a_'+c for c in commodities], superset=sectors)\n",
    "domestic = Group(['d_'+c for c in commodities], superset=sectors)\n",
    "commodity_sectors = Group(['m_'+c for c in commodities]\n",
    "                            +['d_'+c for c in commodities]\n",
    "                            +['a_'+c for c in commodities], superset=sectors)\n",
    "\n",
    "regional_taxes = Group(['tmm_'+r for r in regions]+['tee_'+r for r in regions],superset=sectors)\n",
    "industry_taxes = Group(['tssm_'+c for c in commodities]+['tssd_'+c for c in commodities],superset=sectors)\n",
    "orig_factor_taxes = Group(['tf_'+f for f in orig_factors.clusters],superset=sectors)\n",
    "\n",
    "margins = Group(reduce(lambda x,y:x+y,[['otp_'+r]+['wtp_'+r]+['atp_'+r] for r in regions],[])\n",
    "                +['otp_pvst','wtp_pvst','atp_pvst'],superset=sectors)\n",
    "trade = Group(['ww_'+r for r in regions],superset=sectors)\n",
    "\n",
    "all_but_factors = Group(np.array(['m_'+c for c in commodities]\n",
    "                    +['d_'+c for c in commodities]\n",
    "                    +['a_'+c for c in commodities]\n",
    "                    +['tmm_'+r for r in regions]\n",
    "                    +['tee_'+r for r in regions]\n",
    "                    +['tssm_'+c for c in commodities]\n",
    "                    +['tssd_'+c for c in commodities]\n",
    "                    +reduce(lambda x,y:x+y,[['otp_'+r]+['wtp_'+r]+['atp_'+r] for r in regions],[])\n",
    "                    +['otp_pvst','wtp_pvst','atp_pvst']\n",
    "                    +['ww_'+r for r in regions]\n",
    "                    +['REGHOUS','PRIV','PRODTAX','DIRTAX','GOVT','CGDS',]),superset = sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sectors = Group(np.array(['m_'+c for c in commodities]\n",
    "                    +['d_'+c for c in commodities]\n",
    "                    +['a_'+c for c in commodities]\n",
    "                    +list(factors.elements)\n",
    "                    +['tmm_'+r for r in regions]\n",
    "                    +['tee_'+r for r in regions]\n",
    "                    +['tssm_'+c for c in commodities]\n",
    "                    +['tssd_'+c for c in commodities]\n",
    "                    +['tf_'+f for f in factors]\n",
    "                    +reduce(lambda x,y:x+y,[['otp_'+r]+['wtp_'+r]+['atp_'+r] for r in regions],[])\n",
    "                    +['otp_pvst','wtp_pvst','atp_pvst']\n",
    "                    +['ww_'+r for r in regions]\n",
    "                    +['REGHOUS','PRIV','PRODTAX','DIRTAX','GOVT','CGDS','TRUST']))\n",
    "np.save('scripts/data/ID/sectors.npy',full_sectors.elements)\n",
    "\n",
    "full_imports = Group(['m_'+c for c in commodities], superset=full_sectors)\n",
    "full_activities = Group(['a_'+c for c in commodities], superset=full_sectors)\n",
    "full_domestic = Group(['d_'+c for c in commodities], superset=full_sectors)\n",
    "full_commodity_sectors = Group(['m_'+c for c in commodities]\n",
    "                            +['d_'+c for c in commodities]\n",
    "                            +['d_'+c for c in commodities], superset=full_sectors)\n",
    "\n",
    "factors.set_super(full_sectors)\n",
    "\n",
    "full_regional_taxes = Group(['tmm_'+r for r in regions]+['tee_'+r for r in regions],superset=full_sectors)\n",
    "full_industry_taxes = Group(['tssm_'+c for c in commodities]+['tssd_'+c for c in commodities],superset=full_sectors)\n",
    "full_factor_taxes = Group(['tf_'+f for f in factors],superset=full_sectors)\n",
    "\n",
    "full_margins = Group(reduce(lambda x,y:x+y,[['otp_'+r]+['wtp_'+r]+['atp_'+r] for r in regions],[])\n",
    "                +['otp_pvst','wtp_pvst','atp_pvst'],superset=full_sectors)\n",
    "full_trade = Group(['ww_'+r for r in regions],superset=full_sectors)\n",
    "\n",
    "full_all_but_factors = Group(np.array(['m_'+c for c in commodities]\n",
    "                    +['d_'+c for c in commodities]\n",
    "                    +['a_'+c for c in commodities]\n",
    "                    +['tmm_'+r for r in regions]\n",
    "                    +['tee_'+r for r in regions]\n",
    "                    +['tssm_'+c for c in commodities]\n",
    "                    +['tssd_'+c for c in commodities]\n",
    "                    +reduce(lambda x,y:x+y,[['otp_'+r]+['wtp_'+r]+['atp_'+r] for r in regions],[])\n",
    "                    +['otp_pvst','wtp_pvst','atp_pvst']\n",
    "                    +['ww_'+r for r in regions]\n",
    "                    +['REGHOUS','PRIV','PRODTAX','DIRTAX','GOVT','CGDS']),superset = full_sectors)"
   ]
  },
  {
   "source": [
    "We now load the social accounting matrix. This will require some modification due to the fact that the SAM as computed by GTAP 8 does not balance across all sectors, but this can be easily remedied by the use of a certain global sectors which balance accounts between investors and capital. See (1) for more detail."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 134/134 [03:04<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "sam_list = []\n",
    "for i in tqdm(range(regions.size)):\n",
    "    sam_list.append(np.loadtxt(open(\"data/SAM/\"+str(i+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "sam = np.stack(sam_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 22/22 [00:01<00:00, 19.46it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 57.60it/s]\n"
     ]
    }
   ],
   "source": [
    "EVFA_list = []\n",
    "for i in tqdm(range(factors.size)):\n",
    "    EVFA_list.append(np.loadtxt(open(\"data/EVFA/\"+str(i+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "EVFA = np.stack(EVFA_list,axis=0)\n",
    "\n",
    "VFM_list = []\n",
    "for i in tqdm(range(factors.size)):\n",
    "    VFM_list.append(np.loadtxt(open(\"data/VFM/\"+str(i+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "VFM = np.stack(VFM_list,axis=0)\n",
    "\n",
    "EVOA = np.loadtxt(open(\"data/EVOA.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDEP = sam[:,sectors.ind['CGDS'],orig_factors.clusters.in_superset][:,3]\n",
    "VDEP = np.zeros([regions.size,factors.size])\n",
    "VDEP[:,factors.ind['Capital']] = KDEP"
   ]
  },
  {
   "source": [
    "# Building the SAM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As we mentioned, we will have to harmonize the the social accounting matrix by adjusting some terms."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sam = np.zeros([regions.size,full_sectors.size,full_sectors.size])"
   ]
  },
  {
   "source": [
    "## Things that didn't change"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sam[np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    full_all_but_factors.in_superset,\n",
    "    full_all_but_factors.in_superset\n",
    ")] = sam[np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    all_but_factors.in_superset,\n",
    "    all_but_factors.in_superset\n",
    ")]"
   ]
  },
  {
   "source": [
    "## Things that changed: factors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sam[np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    factors.in_superset,\n",
    "    full_activities.in_superset\n",
    ")] = np.moveaxis(VFM,[0,1,2],[1,2,0])[:,:,:-1]\n",
    "\n",
    "full_sam[np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    full_factor_taxes.in_superset,\n",
    "    full_activities.in_superset\n",
    ")] = np.moveaxis(EVFA-VFM,[0,1,2],[1,2,0])[:,:,:-1]\n",
    "\n",
    "full_sam[:,full_sectors.ind['REGHOUS'],:][np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    factors.in_superset\n",
    ")] = EVOA.T - VDEP\n",
    "\n",
    "full_sam[:,full_sectors.ind['REGHOUS'],:][np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    full_factor_taxes.in_superset\n",
    ")] = np.sum(np.moveaxis(EVFA-VFM,[0,1,2],[1,2,0])[:,:,:-1],axis=2)\n",
    "\n",
    "full_sam[:,full_sectors.ind['DIRTAX'],:][np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    factors.in_superset\n",
    ")] = (np.sum(VFM,axis=1) - EVOA).T\n",
    "\n",
    "full_sam[:,full_sectors.ind['CGDS'],:][np.ix_(\n",
    "    np.arange(regions.size),\n",
    "    factors.in_superset\n",
    ")] = VDEP\n",
    "\n",
    "full_sam[:,full_sectors.ind['REGHOUS'],full_sectors.ind['DIRTAX']] = np.sum((np.sum(VFM,axis=1) - EVOA).T,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = np.sum(full_sam[:,full_sectors.ind['REGHOUS']],axis=1)\\\n",
    "    -full_sam[:,full_sectors.ind['PRIV'],full_sectors.ind['REGHOUS']]\\\n",
    "    -full_sam[:,full_sectors.ind['GOVT'],full_sectors.ind['REGHOUS']]\n",
    "\n",
    "full_sam[:,full_sectors.ind['CGDS'],full_sectors.ind['REGHOUS']] = SAVE*(SAVE>0)\n",
    "full_sam[:,full_sectors.ind['REGHOUS'],full_sectors.ind['CGDS']] = -SAVE*(SAVE<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sam[:,full_sectors.ind['CGDS'],full_trade.in_superset] = np.zeros([regions.size,full_trade.size])\n",
    "full_sam[:,full_trade.in_superset,full_sectors.ind['CGDS']] = np.zeros([regions.size,full_trade.size])\n",
    "\n",
    "cap_in = np.sum(full_sam[:,:,full_sectors.ind['CGDS']],axis=1)\n",
    "cap_out = np.sum(full_sam[:,full_sectors.ind['CGDS'],:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW = cap_out-cap_in\n",
    "full_sam[:,full_sectors.ind['TRUST'],full_sectors.ind['CGDS']] = (FLOW>0)*FLOW\n",
    "full_sam[:,full_sectors.ind['CGDS'],full_sectors.ind['TRUST']] = -FLOW*(FLOW<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('scripts/data/SAM.npy',full_sam)"
   ]
  },
  {
   "source": [
    "# Material Accounting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now we load satellite data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 141.84it/s]\n"
     ]
    }
   ],
   "source": [
    "AREA_list = []\n",
    "for i in tqdm(range(ag.size)):\n",
    "    AREA_list.append(np.loadtxt(open(\"data/AREA/\"+str(i+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "AREA = np.stack(AREA_list,axis=0)\n",
    "np.save('scripts/data/AREA.npy',np.moveaxis(AREA,[0,1,2],[2,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 197.69it/s]\n"
     ]
    }
   ],
   "source": [
    "TONS_list = []\n",
    "for i in tqdm(range(ag.size)):\n",
    "    TONS_list.append(np.loadtxt(open(\"data/TONS/\"+str(i+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "TONS = np.stack(TONS_list,axis=0)\n",
    "np.save('scripts/data/TONS.npy',np.moveaxis(TONS,[0,1,2],[2,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 132.20it/s]\n"
     ]
    }
   ],
   "source": [
    "LCOV_list = []\n",
    "for i in tqdm(range(covers.size)):\n",
    "    LCOV_list.append(np.loadtxt(open(\"data/LCOV/\"+str(i+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "LCOV = np.stack(LCOV_list,axis=0)\n",
    "np.save('scripts/data/LCOV.npy',np.moveaxis(LCOV,[0,1,2],[1,0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 39.49it/s]\n"
     ]
    }
   ],
   "source": [
    "Q_list = []\n",
    "for i in tqdm(range(lab_factors.size)):\n",
    "    Q_list.append(np.loadtxt(open(\"data/Q/\"+str(i+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "Q = np.stack(Q_list,axis=0)\n",
    "np.save('scripts/data/LABF.npy',np.moveaxis(Q,[0,1,2],[2,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 13.03it/s]\n"
     ]
    }
   ],
   "source": [
    "energy_mat_sectors = Group(np.array(['m_'+c for c in ergs]+\n",
    "                                    ['d_'+c for c in ergs]+\n",
    "                                    ['ww_'+r for r in regions]),superset=full_sectors)\n",
    "energy_mat = np.zeros([regions.size,energy_mat_sectors.size,full_sectors.size])\n",
    "\n",
    "EDG = np.loadtxt(open(\"data/EDG.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "EDP = np.loadtxt(open(\"data/EDP.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "EIG = np.loadtxt(open(\"data/EIG.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "EIP = np.loadtxt(open(\"data/EIP.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "\n",
    "for c in tqdm(range(ergs.size)):\n",
    "    EDF_c = np.loadtxt(open(\"data/EDF/\"+str(c+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "    EIF_c = np.loadtxt(open(\"data/EIF/\"+str(c+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "    EXI_c = np.loadtxt(open(\"data/EXI/\"+str(c+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "    for d in (range(commodities.size)):\n",
    "        energy_mat[:,energy_mat_sectors.ind['d_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['a_'+commodities.elements[d]]] = EDF_c[d,:]\n",
    "        energy_mat[:,energy_mat_sectors.ind['m_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['a_'+commodities.elements[d]]] = EIF_c[d,:]\n",
    "    for r in range(regions.size):\n",
    "        energy_mat[:,energy_mat_sectors.ind['d_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['ww_'+regions.elements[r]]] = EXI_c[:,r]\n",
    "        energy_mat[:,energy_mat_sectors.ind['ww_'+regions.elements[r]],\n",
    "                   full_sectors.ind['m_'+ergs.elements[c]]] = EXI_c[r,:]\n",
    "\n",
    "    energy_mat[:,energy_mat_sectors.ind['d_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['CGDS']] = EDF_c[-1,:]\n",
    "    energy_mat[:,energy_mat_sectors.ind['m_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['CGDS']] = EIF_c[-1,:]\n",
    "\n",
    "    energy_mat[:,energy_mat_sectors.ind['d_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['GOVT']] = EDG[c,:]\n",
    "    energy_mat[:,energy_mat_sectors.ind['d_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['PRIV']] = EDP[c,:]\n",
    "    energy_mat[:,energy_mat_sectors.ind['m_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['GOVT']] = EIG[c,:]\n",
    "    energy_mat[:,energy_mat_sectors.ind['m_'+ergs.elements[c]],\n",
    "                   full_sectors.ind['PRIV']] = EIP[c,:]\n",
    "\n",
    "np.save('scripts/data/ERG.npy',energy_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 31.67it/s]\n"
     ]
    }
   ],
   "source": [
    "carbon_mat_sectors = Group(np.array(['m_'+c for c in fuels]+\n",
    "                                    ['d_'+c for c in fuels]),superset=full_sectors)\n",
    "carbon_mat = np.zeros([regions.size,carbon_mat_sectors.size,full_sectors.size])\n",
    "\n",
    "MDG = np.loadtxt(open(\"data/MDG.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "MDP = np.loadtxt(open(\"data/MDP.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "MIG = np.loadtxt(open(\"data/MIG.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "MIP = np.loadtxt(open(\"data/MIP.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "\n",
    "for c in tqdm(range(fuels.size)):\n",
    "    MDF_c = np.loadtxt(open(\"data/MDF/\"+str(c+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "    MIF_c = np.loadtxt(open(\"data/MIF/\"+str(c+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1]\n",
    "    for d in (range(commodities.size)):\n",
    "        carbon_mat[:,carbon_mat_sectors.ind['d_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['a_'+commodities.elements[d]]] = MDF_c[d,:]\n",
    "        carbon_mat[:,carbon_mat_sectors.ind['m_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['a_'+commodities.elements[d]]] = MIF_c[d,:]\n",
    "\n",
    "    carbon_mat[:,carbon_mat_sectors.ind['d_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['CGDS']] = MDF_c[-1,:]\n",
    "    carbon_mat[:,carbon_mat_sectors.ind['m_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['CGDS']] = MIF_c[-1,:]\n",
    "\n",
    "    carbon_mat[:,carbon_mat_sectors.ind['d_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['GOVT']] = MDG[c,:]\n",
    "    carbon_mat[:,carbon_mat_sectors.ind['d_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['PRIV']] = MDP[c,:]\n",
    "    carbon_mat[:,carbon_mat_sectors.ind['m_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['GOVT']] = MIG[c,:]\n",
    "    carbon_mat[:,carbon_mat_sectors.ind['m_'+fuels.elements[c]],\n",
    "                   full_sectors.ind['PRIV']] = MIP[c,:]\n",
    "\n",
    "np.save('scripts/data/CO2.npy',carbon_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP = np.loadtxt(open(\"data/POP.csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('scripts/data/POP.npy',POP.T)"
   ]
  },
  {
   "source": [
    "# Time Series"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here we load the time-series trade data; this will not be use for our current work but may be of use or interest to the reader."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15/15 [00:15<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "years = Group(['Y'+str(j) for j in range(1995,2010)])\n",
    "np.save('scripts/data/ID/years.npy',years.elements)\n",
    "\n",
    "TSERIES_list = []\n",
    "for y in tqdm(years):\n",
    "    YARRAY_list = []\n",
    "    for j in range(commodities.size):\n",
    "        YARRAY_list.append(np.loadtxt(open(\"data/VTTS/\"+y+\"/\"+str(j+1)+\".csv\", \"rb\"), delimiter=\",\", skiprows=1,dtype=str)[:-1,1:-1][:,:-1].astype(float))\n",
    "    TSERIES_list.append(np.stack(YARRAY_list))\n",
    "TSERIES = np.stack(TSERIES_list)\n",
    "\n",
    "np.save('scripts/data/TSERIES.npy',TSERIES)"
   ]
  },
  {
   "source": [
    "## References"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "(1) \"Bound by Chains of Carbon.\" Luke Bergmann. 2013"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('py36': conda)",
   "language": "python",
   "name": "python361064bitpy36conda3f57a39a474d40cab2a06366e779d487"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}